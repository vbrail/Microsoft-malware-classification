{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you will unzip your data you will get the train folder which will contains all the file just seperating the\n",
    "# byte & asm files in seprate folders with foder names as bytefiles and asm files\n",
    "data_files = os.listdir('train')\n",
    "for file in tqdm(data_files):\n",
    "    if (file.endswith(\"bytes\")):\n",
    "        shutil.move('train/'+file,'bytefiles')\n",
    "    if (file.endswith(\"asm\")):\n",
    "        shutil.move('train/'+file,'asmfiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns list of files in directory\n",
    "asmfiles=os.listdir('asmfiles')\n",
    "bytefiles=os.listdir('bytefiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing extension of bytes file and removing the common header in all fies \n",
    "for file in tqdm(bytefiles):\n",
    "    if(file.endswith(\"bytes\")):\n",
    "        file=file.split('.')[0]\n",
    "        text_file = open('bytefiles/'+file+\".txt\", 'w+')\n",
    "        with open('bytefiles/'+file+\".bytes\",\"r\") as fp:\n",
    "            lines=\"\"\n",
    "            for line in fp:\n",
    "                a=line.rstrip().split(\" \")[1:]\n",
    "                b=' '.join(a)\n",
    "                b=b+\"\\n\"\n",
    "                text_file.write(b)\n",
    "            fp.close()\n",
    "            os.remove('bytefiles/'+file+\".bytes\")\n",
    "        text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asm_unigram(files,file_name_suffix):\n",
    "    \"\"\"Prepares unigram BOW for asm files with custom selected words \\\n",
    "        inputs: files:- list of asmfile names \\\n",
    "                file_name_suffix:- used for creating outputfile name with initial as 'asmuni_'+fie_name_suffix \"\"\"\n",
    "    \n",
    "#     list of important words for which we will create BOW\n",
    "    prefixes = ['HEADER:','.text:','.Pav:','.idata:','.data:','.bss:','.rdata:','.edata:','.rsrc:','.tls:','.reloc:','.BSS:','.CODE']\n",
    "    opcodes = ['jmp', 'mov', 'retf', 'push', 'pop', 'xor', 'retn', 'nop', 'sub', 'inc', 'dec', 'add','imul', 'xchg', 'or', 'shr', 'cmp', 'call', 'shl', 'ror', 'rol', 'jnb','jz','rtn','lea','movzx']\n",
    "    keywords = ['.dll','std::',':dword']\n",
    "    registers=['edx','esi','eax','ebx','ecx','edi','ebp','esp','eip']\n",
    "    \n",
    "    fout=open('asmuni_'+file_name_suffix+'.csv',\"w+\")#output file\n",
    "    for file_name in files:\n",
    "#         vectors for counting\n",
    "        prefixes_vec=np.zeros(len(prefixes),dtype=int)\n",
    "        opcodes_vec=np.zeros(len(opcodes),dtype=int)\n",
    "        keyword_vec=np.zeros(len(keywords),dtype=int)\n",
    "        registers_vec=np.zeros(len(registers),dtype=int)\n",
    "        fout.write(file_name.split('.')[0]+\",\")\n",
    "        opcodefile.write(f2+\" \")\n",
    "#         opening file\n",
    "        fin = codecs.open('asmfile/'+file_name,encoding='cp1252',errors ='replace')\n",
    "#     performing countvectorizer\n",
    "        for lines in fin:\n",
    "            line=lines.rstrip().split()\n",
    "            for i in range(len(prefixes)):\n",
    "                if prefixes[i] in line[0]:\n",
    "                    prefixes_vec[i]+=1\n",
    "            for i in range(len(opcodes)):\n",
    "                if any(opcodes[i]==li for li in line[1:]):\n",
    "                    opcodes_vec[i]+=1\n",
    "            for i in range(len(keywords)):\n",
    "                if any(keywords[i]==li  for li in line[1:]):\n",
    "                    keyword_vec[i]+=1\n",
    "            for i in range(len(registers)):\n",
    "                for li in line[1:]:\n",
    "                    if registers[i] in li and ('text' in line[0] or 'CODE' in line[0]):\n",
    "                        registers_vec[i]+=1\n",
    "        fin.close()  \n",
    "#         writing data to file\n",
    "        for i in prefixes_vec:\n",
    "            fout.write(str(i)+\",\")\n",
    "        for i in opcodes_vec:\n",
    "            fout.write(str(i)+\",\")\n",
    "        for i in keyword_vec:\n",
    "            fout.write(str(i)+\",\")\n",
    "        for i in registers_vec:\n",
    "            fout.write(str(i)+\",\")\n",
    "\n",
    "        fout.write(\"\\n\")\n",
    "    fout.close()\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "# for multi core processor i'm running this code on GCP with 8 core vcpu you have to\n",
    "# modify the below code as per your specification\n",
    "    manager=multiprocessing.Manager()\n",
    "    p1=Process(target=asm_unigram,args=(asmfiles[:1358],'1'))\n",
    "    p2=Process(target=asm_unigram,args=(asmfiles[1358:2717],'2'))\n",
    "    p3=Process(target=asm_unigram,args=(asmfiles[2717:4075],'3'))\n",
    "    p4=Process(target=asm_unigram,args=(asmfiles[4075:5434],'4'))\n",
    "    p5=Process(target=asm_unigram,args=(asmfiles[5434:6792],'5'))\n",
    "    p6=Process(target=asm_unigram,args=(asmfiles[6792:8151],'6'))\n",
    "    p7=Process(target=asm_unigram,args=(asmfiles[8151:9509],'7'))\n",
    "    p8=Process(target=asm_unigram,args=(asmfiles[9509:],'8'))\n",
    "    \n",
    "    p1.start()\n",
    "    p2.start()\n",
    "    p3.start()\n",
    "    p4.start()\n",
    "    p5.start()\n",
    "    p6.start()\n",
    "    p7.start()\n",
    "    p8.start()\n",
    "    \n",
    "    p1.join()\n",
    "    p2.join()\n",
    "    p3.join()\n",
    "    p4.join()\n",
    "    p5.join()\n",
    "    p6.join()\n",
    "    p7.join()\n",
    "    p8.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging all the csv created above\n",
    "df= pd.read_csv('asmuni_1.csv')\n",
    "for i in range(2,9):\n",
    "    df.append(pd.read_csv('asmuni_'+str(i)+'.csv'))\n",
    "df.to_csv('asm_unigram.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image feature of asm file \n",
    "# no need to convert into image directly read the bytes in bynary form it will be equivalent to pixels\n",
    "fout = open('asmfiles/'+asmfiles[0],'rb')\n",
    "arr = array.array('B')\n",
    "arr.fromfile(fout,800)\n",
    "fout.close()\n",
    "asm_img_feat_arr = list(arr) \n",
    "for file_name in tqdm(asmfiles[1:]):\n",
    "    fout  = open('asmfiles/'+file_name,'rb')\n",
    "    arr = arr = array.array('B')\n",
    "    arr.fromfile(fout,800)\n",
    "    fout.close()\n",
    "    asm_img_feat_arr = np.vstack((asm_img_feat_arr,list(arr)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram form byte file along with the size of each file as extra feature\n",
    "fout=open('byte_unigram.csv','w+')#output file\n",
    "byte_unigram_array = np.zeros((len(bytefiles),257),dtype=int)#feature array\n",
    "row_ind = 0 # rowindex  of feature array\n",
    "# creating column header\n",
    "fout.write('ID,')\n",
    "for i in range(256):\n",
    "    fout.write(hex(i)[2:]+',')\n",
    "fout.write('??,size\\n')\n",
    "\n",
    "for file in bytefiles:\n",
    "    fout.write(file+\",\")\n",
    "    fin= open('bytefiles/'+file,'r'):\n",
    "#         performing count vectorizer\n",
    "    for lines in fin:\n",
    "        line=lines.lower().rstrip().split(' ')\n",
    "        for code in line:\n",
    "            if code=='??':\n",
    "                byte_unigram_array[row_ind][256]+=1\n",
    "            else:\n",
    "                byte_unigram_array[row_ind][int(code,16)]+=1\n",
    "    fin.close()        \n",
    "    for ele in byte_unigram_array[row_ind]:\n",
    "        fout.write(str(ele)+',')        \n",
    "    fout.write(str(os.stat('bytefiles/'+file).st_size/(1024.0*1024.0)))#writing size\n",
    "    fout.write('\\n')    \n",
    "    row_ind += 1\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary for storing column index of word form '??'.(0-ffff)* , (0-ffff)*'??' & '????'\n",
    "extra_index={}\n",
    "for i in range(256*256):\n",
    "    ky = hex(i)[2:]\n",
    "    \n",
    "index = 256*256\n",
    "for i in range(256):\n",
    "    if i<16:\n",
    "        ky = '0'+hex(i)[2:]+'??'\n",
    "        extra_index[ky]= index\n",
    "    else:\n",
    "        ky = hex(i)[2:]+'??'\n",
    "        extra_index[ky]= index\n",
    "    index +=1\n",
    "    \n",
    "for i in range(256):\n",
    "    if i<16:\n",
    "        ky = '??'+'0'+hex(i)[2:]\n",
    "        extra_index[ky]= index\n",
    "    else:\n",
    "        ky = '??'+hex(i)[2:]\n",
    "        extra_index[ky]= index\n",
    "    index +=1\n",
    "\n",
    "extra_index['????']= index\n",
    "\n",
    "def construct_bigram(file_list,key_suffix):\n",
    "    ''' bigram of byte file'''\n",
    "#     dummy sparse vector\n",
    "    csr_mat = sparse.csr_matrix(np.zeros(66049))\n",
    "    feature_array = None \n",
    "    col_index = 0\n",
    "    row_index = 0\n",
    "    \n",
    "    for file_name in tqdm(file_list):\n",
    "        fout = open('bytefiles/'+file_name,'r')\n",
    "        feature_array = np.zeros(66049) #feature vector\n",
    "        temp_array    = np.zeros(66049) # temp feature vector\n",
    "#         count vectorizer\n",
    "        for line in fout:\n",
    "            word_list = line.rstrip().lower().split(' ')\n",
    "            for hex_pair in zip(word_list,word_list[1:]):\n",
    "                hex_code = \"\".join(hex_pair)\n",
    "                if hex_code in extra_index.keys():\n",
    "                    col_index = extra_index[hex_code]\n",
    "                else:\n",
    "                    col_index = int(hex_code,16)\n",
    "                feature_array[col_index] +=1\n",
    "#         eliminating all the features except top 2000 counts        \n",
    "        for c_i in np.argsort(feature_array)[::-1][:2000] :\n",
    "            temp_array[c_i] = feature_array[c_i] \n",
    "            \n",
    "        fout.close()\n",
    "#         conveting to sparse\n",
    "        feature_array = sparse.csr_matrix(temp_array)\n",
    "#     stacking\n",
    "        csr_mat = sparse.vstack([csr_mat,feature_array])\n",
    "    csr_mat = csr_mat[1:, :]\n",
    "#   saving the final bigram csr matrix\n",
    "    sparse.save_npz('bigram/bigram_array_part_'+key_suffix+\".npz\", csr_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_bigram(bytefiles,'combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_array = sparse.load_npz('bigram/bigram_array_part_combined.npz')\n",
    "# Reading class value And saving it as numpy array\n",
    "trainLabel = pd.read_csv('trainLabels.csv')\n",
    "\n",
    "class_labels=[]\n",
    "for file_name in bytefiles:\n",
    "    class_labels.append(trainLabel[trainLabel['Id']==file_name.split('.')[0]].iloc[0]['Class'])\n",
    "    \n",
    "np.save('class_label.npy',class_labels)\n",
    "# train test split\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(bigram_array, class_labels,stratify=class_labels,test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tunning xgboost for finding important features\n",
    "no_of_trees=[10,50,100,500,1000,2000]\n",
    "train_loss =[]\n",
    "cv_loss=[]\n",
    "for i in no_of_trees:\n",
    "    print('no of estimator :',i)\n",
    "    model=XGBClassifier(n_estimators=i,nthread=-1)\n",
    "    print('Training started...')\n",
    "    st= datetime.now()\n",
    "    model.fit(x_train,y_train)\n",
    "    mt =datetime.now()\n",
    "    print('DONE Time taken :',mt-st)\n",
    "    sig_clf = CalibratedClassifierCV(model, method=\"sigmoid\")\n",
    "    print('Calibrating......')\n",
    "    sig_clf.fit(x_train,y_train)\n",
    "    print('DONE Time taken :',datetime.now()-mt)\n",
    "    pred = sig_clf.predict_proba(x_train)\n",
    "    t_loss = log_loss(y_train, pred, labels=model.classes_, eps=1e-15)\n",
    "    print('Train Loss      :',t_loss)\n",
    "    pred = sig_clf.predict_proba(x_cv)\n",
    "    c_loss = log_loss(y_cv, pred, labels=model.classes_, eps=1e-15)\n",
    "    print('CV    Loss      :',c_loss)\n",
    "    cv_loss.append(c_loss)\n",
    "    train_loss.append(t_loss)\n",
    "    print(\"--------------------------------------------------\")\n",
    "    \n",
    "plt.grid()\n",
    "plt.plot(no_of_trees,train_loss,label='train loss')\n",
    "plt.scatter(no_of_trees,train_loss)\n",
    "plt.plot(no_of_trees,cv_loss,label='CV loss')\n",
    "plt.scatter(no_of_trees,cv_loss)\n",
    "plt.legend()\n",
    "plt.title('Hypreparameter tunning using bigram feature only')\n",
    "plt.xlabel('No of trees')\n",
    "plt.ylabel('log-loss')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with best hyperparameter\n",
    "model=XGBClassifier(n_estimators=1000,nthread=-1)\n",
    "print('Training started...')\n",
    "st= datetime.now()\n",
    "model.fit(x_train,y_train)\n",
    "mt =datetime.now()\n",
    "print('DONE Time taken :',mt-st)\n",
    "sig_clf = CalibratedClassifierCV(model, method=\"sigmoid\")\n",
    "print('Calibrating......')\n",
    "sig_clf.fit(x_train,y_train)\n",
    "print('DONE Time taken :',datetime.now()-mt)\n",
    "pred = sig_clf.predict_proba(x_train)\n",
    "t_loss = log_loss(y_train, pred, labels=model.classes_, eps=1e-15)\n",
    "print('Train Loss      :',t_loss)\n",
    "pred = sig_clf.predict_proba(x_cv)\n",
    "c_loss = log_loss(y_cv, pred, labels=model.classes_, eps=1e-15)\n",
    "print('CV    Loss      :',c_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting top features\n",
    "imp_col =model.get_booster().get_score(importance_type=\"gain\").keys()\n",
    "imp_col_index = []\n",
    "for i in imp_col:\n",
    "    imp_col_index.append(int(i[1:]))\n",
    "top_biram = bigram_array[:,imp_col_index]\n",
    "sparse.save_npz('bigram/top_bigram',top_biram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image feature of byte file\n",
    "fout = open('bytefiles/'+bytefiles[0],'rb')\n",
    "arr = array.array('B')\n",
    "arr.fromfile(fout,800)\n",
    "fout.close()\n",
    "byte_img_feat_arr = list(arr) \n",
    "for file_name in tqdm(bytefiles[1:]):\n",
    "    fout  = open('bytefiles/'+file_name,'rb')\n",
    "    arr = arr = array.array('B')\n",
    "    arr.fromfile(fout,800)\n",
    "    fout.close()\n",
    "    byte_img_feat_arr = np.vstack((byte_img_feat_arr,list(arr)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all features in one \n",
    "# 1> asm unigram 2> asm image 3> bytes unigram with byte file size 4> byte bigram 5> byte image\n",
    "\n",
    "# reading all data\n",
    "data = pd.read_csv('asm_unigram.csv')\n",
    "asm_array = np.zeros(51)\n",
    "for i in tqdm(bytefiles):\n",
    "    asm_array = np.vstack((asm_array,data[data['ID']==i.split('.')[0]].values[0][1:]))\n",
    "asm_array = asm_array[1:]\n",
    "\n",
    "bigram = sparse.load_npz('bigram/top_bigram.npz')\n",
    "\n",
    "byte_image = np.load('byte_img_feat_arr.npy')\n",
    "asm_image  = np.load('asm_img_feat_arr.npy')\n",
    "\n",
    "data = pd.read_csv('byte_unigram.csv')\n",
    "data = data.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "unigram = np.zeros(258)\n",
    "for i in tqdm(bytefiles):\n",
    "    unigram = np.vstack((unigram,data[data['ID']==i.split('.')[0]].values[0][1:]))\n",
    "unigram = unigram[1:]\n",
    "\n",
    "# stacking all data\n",
    "feature_array = np.hstack([unigram,bigram.todense(),asm_array,byte_image,asm_image])\n",
    "feature_array = np.array(feature_array)\n",
    "# saving all data\n",
    "with open('feature_array.pickle', 'wb') as fw:\n",
    "    pickle.dump(feature_array, fw, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  so final dataset is is ready for training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
